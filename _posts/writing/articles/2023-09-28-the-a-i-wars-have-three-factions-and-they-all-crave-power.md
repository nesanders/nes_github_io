---
title: "The A.I. Wars Have Three Factions, and They All Crave Power"
date: 2023-09-28
pin: true
categories: [writing, article]
tags: [ai, democracy, power, the new york times]
description: An article for The New York Times about the emerging factions within the AI community.
authors: [nes, bruce]
---

*This article was written with [Bruce Schneier](https://www.schneier.com) and originally published at [New York Times](https://www.nytimes.com/2023/09/28/opinion/ai-safety-ethics-effective.html) on 2023-09-28.*

<p>
  There is no shortage of researchers and industry titans willing to warn us about the potential destructive power of artificial intelligence. Reading the headlines, one would hope that the rapid gains in A.I. technology have also brought forth a unifying realization of the risks &mdash; and the steps we need to take to mitigate them.
</p>
<p>
  The reality, unfortunately, is quite different. Beneath almost all of the testimony, the manifestoes, the blog posts and the public declarations issued about A.I. are battles among deeply divided factions. Some are concerned about far-future risks that sound like science fiction. Some are genuinely alarmed by the practical problems that chatbots and deepfake video generators are creating right now. Some are motivated by potential business revenue, others by national security concerns.
</p>
<p>
  The result is a cacophony of coded language, contradictory views and provocative policy demands that are undermining our ability to grapple with a technology destined to drive the future of politics, our economy and even our daily lives.
</p>
<p>
  These factions are in dialogue not only with the public but also with one another. Sometimes, they trade letters, opinion essays or social threads outlining their positions and attacking others&rsquo; in public view. More often, they tout their viewpoints without acknowledging alternatives, leaving the impression that their enlightened perspective is the inevitable lens through which to view A.I. But if lawmakers and the public fail to recognize the subtext of their arguments, they risk missing the real consequences of our possible regulatory and cultural paths forward.
</p>




<p>
  To understand the fight and the impact it may have on our shared future, look past the immediate claims and actions of the players to the greater implications of their points of view. When you do, you&rsquo;ll realize this isn&rsquo;t really a debate only about A.I. It&rsquo;s also a contest about control and power, about how resources should be distributed and who should be held accountable.
</p>
<p>
  Beneath this roiling discord is a true fight over the future of society. Should we focus on avoiding the dystopia of mass unemployment, a world where China is the dominant superpower or a society where the worst prejudices of humanity are embodied in opaque algorithms that control our lives? Should we listen to wealthy futurists who discount the importance of climate change because they&rsquo;re already thinking ahead to colonies on Mars? It is critical that we begin to recognize the ideologies driving what we are being told. Resolving the fracas requires us to see through the specter of A.I. to stay true to the humanity of our values.
</p>
<p>
  One way to decode the motives behind the various declarations is through their language. Because language itself is part of their battleground, the different A.I. camps tend not to use the same words to describe their positions. One faction describes the dangers posed by A.I. through the framework of safety, another through ethics or integrity, yet another through security and others through economics. By decoding who is speaking and how A.I. is being described, we can explore where these groups differ and what drives their views.
</p>
##  The Doomsayers

<p>
  The loudest perspective is a frightening, dystopian vision in which A.I. poses an existential risk to humankind, capable of wiping out all life on Earth. A.I., in this vision, emerges as a&nbsp;
  <a title="" href="https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2" target="_blank" rel="noopener noreferrer">
    godlike
  </a>
  , superintelligent, ungovernable entity capable of controlling everything. A.I. could&nbsp;
  <a title="" href="https://www.nytimes.com/2023/06/10/technology/ai-humanity.html">
    destroy humanity
  </a>
  &nbsp;or pose a risk&nbsp;
  <a title="" href="https://www.washingtonpost.com/business/2023/05/30/ai-poses-risk-extinction-industry-leaders-warn/" target="_blank" rel="noopener noreferrer">
    on par with nukes
  </a>
  . If we&rsquo;re not careful, it&nbsp;
  <a title="" href="https://www.thetimes.co.uk/article/rogue-ai-could-kill-everyone-3bsfttpmv" target="_blank" rel="noopener noreferrer">
    could kill everyone
  </a>
  &nbsp;or&nbsp;
  <a title="" href="https://www.spectator.co.uk/article/could-ai-enslave-humanity-before-it-destroys-it-entirely/" target="_blank" rel="noopener noreferrer">
    enslave humanity
  </a>
  . It&rsquo;s likened to monsters like the Lovecraftian shoggoths, artificial servants that rebelled against their creators, or&nbsp;
  <a title="" href="https://nickbostrom.com/ethics/ai" target="_blank" rel="noopener noreferrer">
    paper clip maximizers
  </a>
  &nbsp;that consume all of Earth&rsquo;s resources in a single-minded pursuit of their programmed goal. It sounds like science fiction, but these people are serious, and they mean the words they use.
</p>
<p>
  These are the A.I. safety people, and their ranks include the&nbsp;
  <a title="" href="https://www.businessinsider.com/ai-godfather-top-names-possibilities-dangers-openai-chatgpt-list-2023-8" target="_blank" rel="noopener noreferrer">
    &ldquo;Godfathers of A.I.,&rdquo;
  </a>
  &nbsp;
  <a title="" href="https://www.forbes.com/sites/craigsmith/2023/05/04/geoff-hinton-ais-most-famous-researcher-warns-of-existential-threat/" target="_blank" rel="noopener noreferrer">
    Geoff Hinton
  </a>
  &nbsp;and&nbsp;
  <a title="" href="https://www.economist.com/by-invitation/2023/07/21/one-of-the-godfathers-of-ai-airs-his-concerns" target="_blank" rel="noopener noreferrer">
    Yoshua Bengio
  </a>
  . For many years, these leading lights battled critics who doubted that a computer could ever mimic capabilities of the human mind. Having steamrollered the public conversation by creating large language models like ChatGPT and other A.I. tools capable of increasingly impressive feats, they appear deeply invested in the idea that there is no limit to what their creations will be able to accomplish.
</p>


<p>
  This doomsaying is boosted by a class of tech elite that has enormous power to shape the conversation. And some in this group are animated by the radical effective altruism movement and the associated cause of long-term-ism, which tend to focus on the most extreme catastrophic risks and emphasize the far-future consequences of our actions. These philosophies are hot among the cryptocurrency crowd, like the disgraced former billionaire&nbsp;
  <a title="" href="https://www.nytimes.com/2022/12/09/books/review/effective-altruism-sam-bankman-fried-crypto.html">
    Sam Bankman-Fried
  </a>
  , who at one time possessed sudden wealth in search of a cause.
</p>
<p>
  Reasonable sounding on their face, these ideas can become&nbsp;
  <a title="" href="https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo" target="_blank" rel="noopener noreferrer">
    dangerous
  </a>
  &nbsp;if stretched to their logical extremes. A dogmatic long-termer would willingly sacrifice the well-being of people today to stave off a prophesied extinction event like A.I. enslavement.
</p>
<p>
  Many doomsayers say they are acting rationally, but their hype about hypothetical existential risks amounts to making a misguided bet with our future. In the name of long-term-ism, Elon Musk&nbsp;
  <a title="" href="https://www.businessinsider.com/pronatalism-elon-musk-population-tech-2022-11#:~:text='Pronatalism'%20explained:%20Why%20tech,kids%20to%20save%20the%20world&amp;text=Tech%20titans%20like%20Elon%20Musk%20want%20to%20counteract%20effects%20of,instrumental%20in%20preventing%20existential%20threats." target="_blank" rel="noopener noreferrer">
    reportedly
  </a>
  &nbsp;believes that our society needs to encourage reproduction among those with the greatest culture and intelligence (namely, his ultrarich buddies). And he wants to go further, such as&nbsp;
  <a title="" href="https://www.independent.co.uk/news/world/americas/elon-musk-children-voting-rights-b2369096.html" target="_blank" rel="noopener noreferrer">
    limiting
  </a>
  &nbsp;the right to vote to parents and even&nbsp;
  <a title="" href="https://nymag.com/intelligencer/2022/07/elon-musk-tries-to-populate-mars-himself.html" target="_blank" rel="noopener noreferrer">
    populating Mars
  </a>
  . It&rsquo;s widely believed that&nbsp;
  <a title="" href="https://www.forbes.com/sites/calumchace/2023/04/26/the-ai-suicide-race-with-jaan-tallinn/?sh=5b46828428d6" target="_blank" rel="noopener noreferrer">
    Jaan Tallinn
  </a>
  , the wealthy long-termer who co-founded the&nbsp;
  <a title="" href="https://www.cser.ac.uk/" target="_blank" rel="noopener noreferrer">
    most
  </a>
  &nbsp;
  <a title="" href="https://futureoflife.org/" target="_blank" rel="noopener noreferrer">
    prominent
  </a>
  &nbsp;centers for the study of A.I. safety, has made&nbsp;
  <a title="" href="https://www.cnbc.com/2020/12/29/skype-co-founder-jaan-tallinn-on-3-most-concerning-existential-risks-.html" target="_blank" rel="noopener noreferrer">
    dismissive noises
  </a>
  &nbsp;about climate change because he thinks that it pales in comparison with far-future unknown unknowns like risks from A.I. The technology historian David C. Brock&nbsp;
  <a title="" href="https://lareviewofbooks.org/article/our-censors-ourselves-commercial-content-moderation/" target="_blank" rel="noopener noreferrer">
    calls these fears
  </a>
  &nbsp;&ldquo;wishful worries&rdquo; &mdash; that is, &ldquo;problems that it would be nice to have, in contrast to the actual agonies of the present.&rdquo;
</p>
<p>
  More practically, many of the researchers in this group are&nbsp;
  <a title="" href="https://lareviewofbooks.org/article/our-censors-ourselves-commercial-content-moderation/" target="_blank" rel="noopener noreferrer">
    proceeding
  </a>
  &nbsp;full steam ahead in developing A.I., demonstrating how unrealistic it is to simply hit&nbsp;
  <a title="" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" target="_blank" rel="noopener noreferrer">
    pause
  </a>
  &nbsp;on technological development. But the roboticist Rodney Brooks has&nbsp;
  <a title="" href="https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/" target="_blank" rel="noopener noreferrer">
    pointed out
  </a>
  &nbsp;that we will see the existential risks coming, the dangers will not be sudden and we will have time to change course. While we shouldn&rsquo;t dismiss the Hollywood nightmare scenarios out of hand, we must balance them with the potential benefits of A.I. and, most important, not allow them to strategically distract from more immediate concerns. Let&rsquo;s not let apocalyptic prognostications overwhelm us and smother the momentum we need to develop critical guardrails.
</p>

##  The Reformers

<p>
  While the doomsayer faction focuses on the far-off future, its most prominent opponents are focused on the here and now. We agree with this group that there&rsquo;s plenty already happening to cause concern:&nbsp;
  <a title="" href="https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/" target="_blank" rel="noopener noreferrer">
    Racist
  </a>
  &nbsp;policing and legal systems that disproportionately arrest and punish people of color.&nbsp;
  <a title="" href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G" target="_blank" rel="noopener noreferrer">
    Sexist
  </a>
  &nbsp;labor systems that rate feminine-coded r&eacute;sum&eacute;s lower. Superpower nations automating&nbsp;
  <a title="" href="https://www.thenation.com/article/world/artificial-intelligence-us-military/" target="_blank" rel="noopener noreferrer">
    military interventions
  </a>
  &nbsp;as tools of imperialism and, someday,&nbsp;
  <a title="" href="https://www.stopkillerrobots.org/" target="_blank" rel="noopener noreferrer">
    killer robots
  </a>
  .
</p>



<p>
  The alternative to the end-of-the-world, existential risk narrative is a distressingly familiar vision of dystopia: a society in which humanity&rsquo;s worst instincts are encoded into and enforced by machines. The doomsayers think A.I. enslavement looks like the Matrix; the reformers point to modern-day contractors doing traumatic work&nbsp;
  <a title="" href="https://www.wsj.com/articles/chatgpt-openai-content-abusive-sexually-explicit-harassment-kenya-workers-on-human-workers-cf191483" target="_blank" rel="noopener noreferrer">
    at low pay
  </a>
  &nbsp;for OpenAI in Kenya.
</p>
<p>
  Propagators of these A.I. ethics concerns &mdash; like&nbsp;
  <a title="" href="https://mitpress.mit.edu/9780262047654/more-than-a-glitch/" target="_blank" rel="noopener noreferrer">
    Meredith Broussard
  </a>
  ,&nbsp;
  <a title="" href="https://nyupress.org/9781479837243/algorithms-of-oppression/" target="_blank" rel="noopener noreferrer">
    Safiya Umoja Noble
  </a>
  ,&nbsp;
  <a title="" href="https://www.rummanchowdhury.com/" target="_blank" rel="noopener noreferrer">
    Rumman Chowdhury
  </a>
  &nbsp;and&nbsp;
  <a title="" href="https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/" target="_blank" rel="noopener noreferrer">
    Cathy O&rsquo;Neil
  </a>
  &nbsp;&mdash; have been raising the alarm on inequities coded into A.I. for years. Although we don&rsquo;t have a census, it&rsquo;s noticeable that&nbsp;
  <a title="" href="https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/" target="_blank" rel="noopener noreferrer">
    many leaders
  </a>
  &nbsp;in this cohort are people of color, women and people who identify as L.G.B.T.Q. They are often motivated by insight into what it feels like to be on the wrong end of algorithmic oppression and by a connection to the communities most vulnerable to the misuse of new technology. Many in this group take an explicitly social perspective: When&nbsp;
  <a title="" href="https://www.media.mit.edu/people/joyab/overview/" target="_blank" rel="noopener noreferrer">
    Joy Buolamwini
  </a>
  &nbsp;founded an organization to fight for equitable A.I., she called it the&nbsp;
  <a title="" href="https://www.ajl.org/" target="_blank" rel="noopener noreferrer">
    Algorithmic Justice League
  </a>
  . Ruha Benjamin called her organization the Ida B. Wells&nbsp;
  <a title="" href="https://www.thejustdatalab.com/" target="_blank" rel="noopener noreferrer">
    Just Data Lab
  </a>
  .
</p>
<p>
  Others frame efforts to reform A.I. in terms of integrity, calling for Big Tech to adhere to an&nbsp;
  <a title="" href="https://time.com/6104899/facebook-reckoning-frances-haugen/" target="_blank" rel="noopener noreferrer">
    oath
  </a>
  &nbsp;to consider the benefit of the broader public alongside &mdash; or even above &mdash; their self-interest. They&nbsp;
  <a title="" href="https://www.cbsnews.com/news/facebook-whistleblower-frances-haugen-misinformation-public-60-minutes-2021-10-03/" target="_blank" rel="noopener noreferrer">
    point to
  </a>
  &nbsp;social media companies&rsquo; failure to control hate speech or how online misinformation can undermine democratic elections. Adding urgency for this group is that the very companies driving the A.I. revolution have, at times, been eliminating safeguards. A signal moment came when Timnit Gebru, a co-leader of Google&rsquo;s A.I. ethics team, was dismissed for&nbsp;
  <a title="" href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/" target="_blank" rel="noopener noreferrer">
    pointing out
  </a>
  &nbsp;the risks of developing ever-larger A.I. language models.
</p>
<p>
  While doomsayers and reformers share the concern that A.I. must align with human interests, reformers tend to push back hard against the doomsayers&rsquo; focus on the distant future. They want to wrestle the attention of regulators and advocates back toward present-day harms that are exacerbated by A.I. misinformation, surveillance and inequity. Integrity&nbsp;
  <a title="" href="https://integrityinstitute.org/" target="_blank" rel="noopener noreferrer">
    experts
  </a>
  &nbsp;call for the development of responsible A.I., for civic education to ensure A.I. literacy and for keeping humans front and center in A.I. systems.
</p>
<p>
  This group&rsquo;s concerns are well documented and urgent &mdash; and far older than modern A.I. technologies. Surely, we are a civilization big enough to tackle more than one problem at a time; even those worried that A.I. might kill us in the future should still demand that it not profile and exploit us in the present.
</p>




##  The Warriors

<p>
  Other groups of prognosticators cast the rise of A.I. through the language of competitiveness and national security. One version has a post-9/11 ring to it &mdash; a world where terrorists, criminals and psychopaths have unfettered access to technologies of mass destruction. Another version is a Cold War narrative of the United States losing an&nbsp;
  <a title="" href="https://www.project-syndicate.org/commentary/china-versus-america-ai-race-pandemic-by-eric-schmidt-and-graham-allison-2020-08" target="_blank" rel="noopener noreferrer">
    A.I. arms race
  </a>
  &nbsp;with China and its surveillance-rich society.
</p>
<p>
  Some arguing from this perspective are acting on genuine national security concerns, and others have a simple motivation: money. These perspectives serve the interests of American tech tycoons as well as the government agencies and defense contractors they are&nbsp;
  <a title="" href="https://www.nytimes.com/2022/12/07/business/pentagon-cloud-contracts-jwcc.html">
    intertwined
  </a>
  &nbsp;with.
</p>
<p>
  OpenAI&rsquo;s&nbsp;
  <a title="" href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html">
    Sam Altman
  </a>
  &nbsp;and Meta&rsquo;s&nbsp;
  <a title="" href="https://www.nytimes.com/2019/03/30/technology/mark-zuckerberg-facebook-regulation-explained.html">
    Mark Zuckerberg
  </a>
  , both of whom lead dominant A.I. companies, are pushing for A.I. regulations that they say will protect us from criminals and terrorists. Such regulations would be expensive to comply with and are likely to&nbsp;
  <a title="" href="https://www.csis.org/analysis/managing-existential-risk-ai-without-undercutting-innovation" target="_blank" rel="noopener noreferrer">
    preserve
  </a>
  &nbsp;the market position of leading A.I. companies while restricting competition from start-ups. In the lobbying battles over Europe&rsquo;s trailblazing A.I. regulatory framework, U.S. megacompanies&nbsp;
  <a title="" href="https://corporateeurope.org/sites/default/files/2023-03/The%20Lobbying%20Ghost%20in%20the%20Machine.pdf" target="_blank" rel="noopener noreferrer">
    pleaded
  </a>
  &nbsp;to exempt their general purpose A.I. from the tightest regulations, and&nbsp;
  <a title="" href="https://www.brookings.edu/articles/the-eus-attempt-to-regulate-open-source-ai-is-counterproductive/" target="_blank" rel="noopener noreferrer">
    whether
  </a>
  &nbsp;and how to apply high-risk compliance expectations on noncorporate open-source models emerged as a key point of debate. All the while, some of the moguls investing in upstart companies are fighting the regulatory tide. The&nbsp;
  <a title="" href="https://inflection.ai/reid-hoffman" target="_blank" rel="noopener noreferrer">
    Inflection AI
  </a>
  &nbsp;co-founder Reid Hoffman&nbsp;
  <a title="" href="https://www.ft.com/content/02302d04-846e-4d8a-a868-de895dde5a01" target="_blank" rel="noopener noreferrer">
    argued
  </a>
  , &ldquo;The answer to our challenges is not to slow down technology but to accelerate it.&rdquo;
</p>
<p>
  Any technology critical to national defense usually has an&nbsp;
  <a title="" href="https://foreignpolicy.com/2023/06/19/us-china-ai-race-regulation-artificial-intelligence/" target="_blank" rel="noopener noreferrer">
    easier time
  </a>
  &nbsp;avoiding oversight, regulation and limitations on profit. Any readiness gap in our military&nbsp;
  <a title="" href="https://www.defensenews.com/opinion/commentary/2022/05/23/the-us-defense-budgets-latest-casualty-is-readiness/" target="_blank" rel="noopener noreferrer">
    demands
  </a>
  &nbsp;urgent budget increases, funds distributed to the military branches and their contractors, because we may soon be called upon to fight. Tech moguls like Google&rsquo;s former chief executive&nbsp;
  <a title="" href="https://www.bloomberg.com/news/articles/2023-09-08/google-ex-ceo-eric-schmidt-influences-ai-policy-with-27-billion-fortune" target="_blank" rel="noopener noreferrer">
    Eric Schmidt
  </a>
  , who has the ear of many lawmakers, signal to American policymakers about the Chinese threat even as they&nbsp;
  <a title="" href="https://www.vox.com/recode/2022/6/9/23160588/eric-schmidt-americas-frontier-fund-google-alphabet-tech-government-revolving-door" target="_blank" rel="noopener noreferrer">
    invest in
  </a>
  &nbsp;U.S. national security concerns.
</p>
<p>
  The warriors&rsquo; narrative seems to misrepresent that science and engineering are different from what they were during the mid-20th century. A.I. research is fundamentally international; no one country will win a monopoly. And while national security is important to consider, we must also be mindful of self-interest of those positioned to benefit financially.
</p>


<p>
  As the science-fiction&nbsp;
  <a title="" href="https://www.nytimes.com/2023/02/26/opinion/microsoft-bing-sydney-artificial-intelligence.html">
    author Ted Chiang
  </a>
  &nbsp;has said, fears about the existential risks of A.I. are really fears about the threat of uncontrolled capitalism, and dystopias like the paper clip maximizer are just caricatures of every start-up&rsquo;s business plan. Cosma Shalizi and Henry Farrell further&nbsp;
  <a title="" href="https://www.economist.com/by-invitation/2023/06/21/artificial-intelligence-is-a-familiar-looking-monster-say-henry-farrell-and-cosma-shalizi" target="_blank" rel="noopener noreferrer">
    argue
  </a>
  &nbsp;that &ldquo;we&rsquo;ve lived among shoggoths for centuries, tending to them as though they were our masters&rdquo; as monopolistic platforms devour and exploit the totality of humanity&rsquo;s labor and ingenuity for their own interests. This dread applies as much to our future with A.I. as it does to our past and present with corporations.
</p>
<p>
  Regulatory solutions do not need to&nbsp;
  <a title="" href="https://www.belfercenter.org/publication/we-dont-need-reinvent-our-democracy-save-it-ai" target="_blank" rel="noopener noreferrer">
    reinvent
  </a>
  &nbsp;the wheel. Instead, we need to double down on the rules that we know limit corporate power. We need to get more serious about establishing good and effective governance on all the issues we lost track of while we were becoming obsessed with A.I., China and the&nbsp;
  <a title="" href="https://www.nytimes.com/2023/07/01/technology/elon-musk-mark-zuckerberg-cage-match.html">
    fights
  </a>
  &nbsp;picked among robber barons.
</p>
<p>
  By analogy to the health care sector, we need an&nbsp;
  <a title="" href="https://slate.com/technology/2023/04/ai-public-option.html" target="_blank" rel="noopener noreferrer">
    A.I. public option
  </a>
  &nbsp;to truly keep A.I. companies in check. A&nbsp;
  <a title="" href="https://foreignpolicy.com/2023/06/12/ai-regulation-technology-us-china-eu-governance/" target="_blank" rel="noopener noreferrer">
    publicly directed A.I.
  </a>
  &nbsp;development project would serve to counterbalance for-profit corporate A.I. and help ensure an even playing field for access to the 21st century&rsquo;s key technology while offering a platform for the ethical development and use of A.I.
</p>
<p>
  Also, we should embrace the humanity behind A.I. We can hold founders and corporations accountable by mandating greater A.I. transparency in the development stage, in addition to applying legal standards for actions associated with A.I. Remarkably, this is something that both the&nbsp;
  <a title="" href="https://www.warren.senate.gov/newsroom/press-releases/warren-introduces-accountable-capitalism-act" target="_blank" rel="noopener noreferrer">
    left
  </a>
  &nbsp;and the&nbsp;
  <a title="" href="https://www.penguinrandomhouse.com/books/708057/tyranny-inc-by-sohrab-ahmari/" target="_blank" rel="noopener noreferrer">
    right
  </a>
  &nbsp;can agree on.
</p>
<p>
  Ultimately, we need to make sure the network of laws and regulations that govern our collective behavior is knit more strongly, with fewer gaps and greater ability to hold the powerful accountable, particularly in those areas most sensitive to our democracy and environment. As those with power and privilege seem poised to harness A.I. to accumulate much more or pursue extreme ideologies, let&rsquo;s think about how we can constrain their influence in the public square rather than cede our attention to their most bombastic nightmare visions for the future.
</p>
