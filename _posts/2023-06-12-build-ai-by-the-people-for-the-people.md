---
title: "Build AI by the People, for the People"
date: 2023-06-12
pin: false
categories: [writing, article]
tags: [democracy, ai, public ai, foreign policy]
description: An article for Foreign Policy making the case for Public AI.
authors: [nes, bruce]
---

*This article was written with [Bruce Schneier](https://www.schneier.com) and originally published at [Foreign Policy](https://foreignpolicy.com/2023/06/12/ai-regulation-technology-us-china-eu-governance/) on 2023-06-12.*

<p>Artificial intelligence will bring great benefits to all of humanity. But do we really want to entrust this revolutionary technology solely to a small group of U.S. tech companies?</p>
<p>Silicon Valley has produced no small number of moral disappointments. Google <a href="https://gizmodo.com/google-removes-nearly-all-mentions-of-dont-be-evil-from-1826153393" target="_blank"> retired </a> its &ldquo;don&rsquo;t be evil&rdquo; pledge before <a href="https://www.washingtonpost.com/technology/2020/12/23/google-timnit-gebru-ai-ethics/" target="_blank"> firing </a> its star ethicist. Self-proclaimed &ldquo; <a href="https://www.npr.org/2022/10/08/1127689351/elon-musk-calls-himself-a-free-speech-absolutist-what-could-twitter-look-like-un" target="_blank"> free speech absolutist </a> &rdquo; Elon Musk bought Twitter in order to censor political <a href="https://www.theatlantic.com/ideas/archive/2023/04/elon-musk-twitter-free-speech-matt-taibbi-substack/673698/" target="_blank"> speech </a> , retaliate against journalists, and <a href="https://apnews.com/article/twitter-russia-china-elon-musk-ukraine-2eedeabf7d555dc1d0a68b3724cfdd55" target="_blank"> ease </a> access to the platform for Russian and Chinese propagandists. Facebook <a href="https://www.nytimes.com/2018/11/14/technology/facebook-crisis-mark-zuckerberg-sheryl-sandberg.html" target="_blank"> lied </a> about how it enabled Russian interference in the 2016 U.S. presidential election and <a href="https://www.businessinsider.com/facebook-george-soros-critics-nyt-2018-11" target="_blank"> paid </a> a public relations firm to blame Google and George Soros instead.</p>
<p>These and countless other ethical lapses should prompt us to consider whether we want to give technology companies further abilities to learn our personal details and influence our day-to-day decisions. Tech companies can already access our daily whereabouts and search queries. Digital devices monitor more and more aspects of our lives: We have cameras in our homes and heartbeat sensors on our wrists sending what they detect to Silicon Valley.</p>
<p>Now, tech giants are developing ever more powerful AI systems that don&rsquo;t merely monitor you; they actually interact with you&mdash;and with others <a href="https://danielmiessler.com/p/ais-next-big-thing-is-digital-assistants/" target="_blank"> on your behalf </a> . If searching on Google in the 2010s was like being watched on a security camera, then using AI in the late 2020s will be like having a butler. You will willingly include them in every conversation you have, everything you write, every item you shop for, every want, every fear, everything. It will never forget. And, despite your reliance on it, it will be surreptitiously working to further the interests of one of these for-profit corporations.</p>
<p>There&rsquo;s a reason Google, Microsoft, Facebook, and other large tech companies are leading the AI revolution: Building a competitive large language model (LLM) like the one powering ChatGPT is <a href="https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems" target="_blank"> incredibly expensive </a> . It requires upward of <a href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/" target="_blank"> $100 million </a> in computational costs for a single model training run, in addition to access to large amounts of data. It also requires technical expertise, which, while <a href="https://www.theguardian.com/technology/2023/mar/07/techscape-meta-leak-llama-chatgpt-ai-crossroads" target="_blank"> increasingly </a> <a href="https://huggingface.co/bigscience/bloom" target="_blank"> open </a> and <a href="https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century" target="_blank"> available </a> , remains heavily <a href="https://www.forbes.com/sites/groupthink/2018/02/09/traditional-recruiting-isnt-enough-how-ai-is-changing-the-rules-in-the-human-capital-market/" target="_blank"> concentrated </a> in a small handful of companies. Efforts to disrupt the AI oligopoly by funding start-ups are self-defeating as Big Tech profits from the cloud computing services and AI <a href="https://blogs.nvidia.com/blog/2023/03/13/what-are-foundation-models/" target="_blank"> models </a> powering those start-ups&mdash;and often ends up acquiring the start-ups themselves.</p>
<p>Yet corporations aren&rsquo;t the only entities large enough to absorb the cost of large-scale model training. Governments can do it, too. It&rsquo;s time to start taking AI development out of the exclusive hands of private companies and <a href="https://slate.com/technology/2023/04/ai-public-option.html" target="_blank"> bringing it </a> into the public sector. The United States needs a government-funded and -directed AI program to develop widely reusable models in the public interest, guided by technical expertise housed in federal agencies.</p>
<p>So far, the AI regulation <a href="https://foreignpolicy.com/2023/06/06/how-to-regulate-ai/" target="_blank"> debate </a> in Washington has focused on the <a href="https://www.axios.com/2023/05/16/openai-ceo-sam-altman-artificial-intelligence-congress" target="_blank"> governance </a> of private-sector activity&mdash;which the U.S. Congress is in <a href="https://www.nytimes.com/2023/03/03/business/dealbook/lawmakers-ai-regulations.html" target="_blank"> no hurry </a> to advance. Congress should not only hurry up and push AI regulation forward but also go one step further and develop its own programs for AI. Legislators should reframe the AI debate from one about public regulation to one about public development.</p>
<p>The AI development program could be responsive to public input and subject to political oversight. It could be directed to respond to critical issues such as <a href="https://www.nytimes.com/2023/04/11/podcasts/ezra-klein-podcast-transcript-alondra-nelson.html" target="_blank"> privacy protection </a> , <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/" target="_blank"> underpaid </a> tech workers, AI&rsquo;s horrendous <a href="https://www.nature.com/articles/d41586-023-00843-2" target="_blank"> carbon emissions </a> , and the <a href="https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data" target="_blank"> exploitation </a> of unlicensed data. Compared to keeping AI in the hands of morally dubious tech companies, the public alternative is better both ethically and economically. And the switch should take place soon: By the time AI becomes critical infrastructure, essential to large swaths of economic activity and daily life, it will be too late to get started.</p>
<p>Other countries are <a href="https://www.nytimes.com/2023/03/03/business/dealbook/lawmakers-ai-regulations.html" target="_blank"> already there </a> . China has heavily <a href="https://www.hhs.se/contentassets/bc962221471a415ba8ac01fbbf160277/chinas-ai-ecosystem-nov-2022.pdf" target="_blank"> prioritized </a> public investment in AI research and development by betting on a <a href="https://www.newamerica.org/cybersecurity-initiative/digichina/blog/drafting-chinas-national-ai-team-governance/" target="_blank"> handpicked </a> set of giant companies that are ostensibly private but widely <a href="https://www.theguardian.com/world/2019/jul/25/china-business-xi-jinping-communist-party-state-private-enterprise-huawei" target="_blank"> understood </a> to be an extension of the state. The government has tasked Alibaba, Huawei, and others with creating products that support the larger ecosystem of state surveillance and authoritarianism.</p>
<p>The European Union is also aggressively pushing AI development. The European Commission <a href="https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence" target="_blank"> already invests </a> 1 billion euros per year in AI, with a plan to increase that figure to 20 billion euros annually by 2030. The money goes to a continent-wide network of public research labs, universities, and private companies jointly working on various parts of AI. The Europeans&rsquo; focus is on knowledge transfer, developing the technology sector, use of AI in public administration, mitigating safety risks, and preserving fundamental rights. The EU also continues to be at the cutting edge of aggressively <a href="https://techcrunch.com/2023/04/13/generative-ai-gdpr-enforcement/" target="_blank"> regulating </a> both data and AI.</p>
<p>Neither the Chinese nor the European model is necessarily right for the United States. State control of private enterprise remains anathema in American political culture and would struggle to gain mainstream traction. The tech companies&mdash;and their supporters in both U.S. political parties&mdash;are opposed to robust public governance of AI. But Washington can take inspiration from China and Europe&rsquo;s long-range planning and leadership on regulation and public investment. With boosters pointing to hundreds of <a href="https://www.fool.com/investing/2023/04/28/generative-ai-add-200-trillion-2030-2-stocks-buy/" target="_blank"> trillions </a> of dollars of global economic value associated with AI, the stakes of international competition are compelling. As in energy and medical research, which have their own federal agencies in the Department of Energy and the National Institutes of Health, respectively, there is a place for AI research and development inside government.</p>
<p>Beside the moral argument against letting private companies develop AI, there&rsquo;s a strong economic argument in favor of a public option as well. A publicly funded LLM could serve as an open platform for innovation, helping any small business, nonprofit, or individual entrepreneur to build AI-assisted applications.</p>
<p>There&rsquo;s also a practical argument. Building AI is within public reach because governments don&rsquo;t need to own and operate the entire AI supply chain. Chip and computer production, cloud data centers, and various value-added applications&mdash;such as those that integrate AI with consumer electronics devices or entertainment software&mdash;do not need to be publicly controlled or funded.</p>
<p>One reason to be skeptical of public funding for AI is that it might result in a lower quality and slower innovation, given greater ethical scrutiny, political constraints, and fewer incentives due to a lack of market competition. But even if that is the case, it would be worth broader access to the most important technology of the 21st century. And it is by no means certain that public AI has to be at a disadvantage. The open-source community is <a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither" target="_blank"> proof </a> that it&rsquo;s not always private companies that are the most innovative.</p>
<p>Those who worry about the quality trade-off might suggest a public buyer model, whereby Washington licenses or buys private language models from Big Tech instead of developing them itself. But that doesn&rsquo;t go far enough to ensure that the tools are aligned with public priorities and responsive to public needs. It would not give the public detailed insight into or control of the inner workings and training procedures for these models, and it would still require <a href="https://www.cms.gov/Regulations-and-Guidance/Regulations-and-Guidance" target="_blank"> strict and complex regulation </a> .</p>
<p>There is political will to take action to develop AI via public, rather than private, funds&mdash;but this does not yet equate to the will to create a fully public AI development agency. A <a href="https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf" target="_blank"> task force </a> created by Congress recommended in January a $2.6 billion federal investment in computing and data resources to prime the AI research ecosystem in the United States. But this investment would largely serve to advance the interests of Big Tech, leaving the opportunity for public ownership and oversight unaddressed.</p>
<p>Nonprofit and academic organizations have already <a href="https://techcrunch.com/2022/07/12/a-year-in-the-making-bigsciences-ai-language-model-is-finally-available/" target="_blank"> created </a> open-access LLMs. While these should be celebrated, they are not a substitute for a public option. Nonprofit projects are still beholden to private interests, even if they are benevolent ones. These private interests can change without public input, as when OpenAI effectively <a href="https://www.vice.com/en/article/5d3naz/openai-is-now-everything-it-promised-not-to-be-corporate-closed-source-and-for-profit" target="_blank"> abandoned </a> its nonprofit origins, and we can&rsquo;t be sure that their founding intentions or operations will <a href="https://civictech.guide/graveyard/" target="_blank"> survive </a> market pressures, fickle donors, and changes in leadership.</p>
<p>The U.S. government is by no means a perfect beacon of transparency, a secure and responsible store of our data, or a genuine reflection of the public&rsquo;s interests. But the risks of placing AI development entirely in the hands of demonstrably untrustworthy Silicon Valley companies are too high. AI will impact the public like few other technologies, so it should also be developed by the public.</p>
